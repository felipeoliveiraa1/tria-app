import { useCallback, useRef, useState } from 'react'
import { useRecordingStore } from '../store/recording-store'
import { useAudioControl } from './use-audio-control'

export function useImprovedSTT() {
  const [isConnected, setIsConnected] = useState(false)
  const [useOpenAI, setUseOpenAI] = useState(false)
  const processingRef = useRef(false)
  const lastChunkTime = useRef(0)
  
  const { 
    addPartialText, 
    addFinalSegment, 
    setRealtimeConnected 
  } = useRecordingStore()
  
  const audioControl = useAudioControl()

  // Verificar se OpenAI estÃ¡ disponÃ­vel
  const checkOpenAI = useCallback(async () => {
    try {
      const response = await fetch('/api/anamnese/ingest', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ transcriptChunk: 'teste' })
      })
      const data = await response.json()
      const hasOpenAI = !data.mock
      console.log('ðŸ” OpenAI disponÃ­vel:', hasOpenAI)
      setUseOpenAI(hasOpenAI)
      return hasOpenAI
    } catch (error) {
      console.warn('Erro ao verificar OpenAI, usando Web Speech API')
      setUseOpenAI(false)
      return false
    }
  }, [])

  // Transcrever com OpenAI
  const transcribeWithOpenAI = useCallback(async (audioBlob: Blob) => {
    if (processingRef.current || audioBlob.size < 1000) {
      console.log('â­ï¸ Pular chunk muito pequeno ou jÃ¡ processando')
      return
    }

    try {
      processingRef.current = true
      console.log('ðŸ”„ Enviando para OpenAI:', audioBlob.size, 'bytes')
      
      const formData = new FormData()
      formData.append('audio', audioBlob, 'audio.webm')
      
      const response = await fetch('/api/transcribe', {
        method: 'POST',
        body: formData
      })
      
      if (response.ok) {
        const data = await response.json()
        if (data.text && data.text.trim().length > 0 && !data.mock && !data.filtered) {
          console.log('âœ… TranscriÃ§Ã£o OpenAI:', data.text)
          addFinalSegment({
            text: data.text.trim(),
            startMs: Date.now(),
            endMs: Date.now() + 1000,
            confidence: 0.9,
            isPartial: false
          })
        } else if (data.filtered) {
          console.warn('âš ï¸ TranscriÃ§Ã£o filtrada')
        }
      }
    } catch (error) {
      console.error('âŒ Erro na transcriÃ§Ã£o OpenAI:', error)
    } finally {
      processingRef.current = false
    }
  }, [addFinalSegment])

  // Conectar transcriÃ§Ã£o
  const connect = useCallback(async () => {
    try {
      console.log('ðŸ”— Conectando transcriÃ§Ã£o...')
      
      // Carregar dispositivos e verificar OpenAI
      await audioControl.loadDevices()
      const shouldUseOpenAI = await checkOpenAI()
      
      // Iniciar captura de Ã¡udio
      const stream = await audioControl.startAudioCapture()
      
      if (shouldUseOpenAI) {
        console.log('ðŸ¤– Usando OpenAI Whisper')
        
        // Configurar gravaÃ§Ã£o em chunks
        const recorder = audioControl.createRecorder(stream, (blob) => {
          // SÃ³ processar se hÃ¡ atividade de Ã¡udio
          if (audioControl.hasAudioActivity(25)) {
            const now = Date.now()
            if (now - lastChunkTime.current > 2000) { // MÃ­nimo 2s entre chunks
              lastChunkTime.current = now
              transcribeWithOpenAI(blob)
            }
          } else {
            console.log('ðŸ”‡ Sem atividade de Ã¡udio, pular chunk')
          }
        })
        
        // Iniciar gravaÃ§Ã£o em chunks de 4 segundos
        recorder.start()
        const interval = setInterval(() => {
          if (recorder.state === 'recording') {
            recorder.stop()
            setTimeout(() => {
              if (recorder.state === 'inactive') {
                recorder.start()
              }
            }, 300)
          }
        }, 4000)
        
        // Cleanup
        return () => {
          clearInterval(interval)
          if (recorder.state !== 'inactive') {
            recorder.stop()
          }
        }
      } else {
        console.log('ðŸŽ¤ Usando Web Speech API')
        
        // Fallback para Web Speech API
        if (!('webkitSpeechRecognition' in window || 'SpeechRecognition' in window)) {
          throw new Error('Speech Recognition nÃ£o suportado')
        }
        
        const SpeechRecognition = (window as any).webkitSpeechRecognition || (window as any).SpeechRecognition
        const recognition = new SpeechRecognition()
        
        recognition.continuous = true
        recognition.interimResults = true
        recognition.lang = 'pt-BR'
        recognition.maxAlternatives = 1
        
        recognition.onresult = (event: any) => {
          let finalTranscript = ''
          let interimTranscript = ''
          
          for (let i = event.resultIndex; i < event.results.length; i++) {
            const transcript = event.results[i][0].transcript
            
            if (event.results[i].isFinal) {
              finalTranscript += transcript
            } else {
              interimTranscript += transcript
            }
          }
          
          if (interimTranscript) {
            addPartialText(interimTranscript)
          }
          
          if (finalTranscript && finalTranscript.trim().length > 0) {
            const cleanText = finalTranscript.trim()
            
            // Filtros bÃ¡sicos
            const invalidPatterns = [
              /Ð´ÑÐºÑƒÑŽ|ÑÐ¿Ð°ÑÐ¸Ð±Ð¾|thank you|share.*video.*social.*media/i,
              /^[\s\p{P}]*$/u,
              /[^\p{L}\p{N}\p{P}\p{Z}\s]/u
            ]
            
            const isValid = !invalidPatterns.some(pattern => pattern.test(cleanText)) && 
                           cleanText.length < 200
            
            if (isValid) {
              console.log('âœ… TranscriÃ§Ã£o Web Speech:', cleanText)
              addFinalSegment({
                text: cleanText,
                startMs: Date.now() - 2000,
                endMs: Date.now(),
                confidence: 0.8,
                isPartial: false
              })
            }
          }
        }
        
        recognition.onerror = (event: any) => {
          console.error('âŒ Erro Speech Recognition:', event.error)
        }
        
        recognition.start()
        
        return () => {
          recognition.stop()
        }
      }
    } catch (error) {
      console.error('âŒ Erro ao conectar transcriÃ§Ã£o:', error)
      setRealtimeConnected(false)
      throw error
    }
  }, [audioControl, checkOpenAI, transcribeWithOpenAI, addPartialText, addFinalSegment, setRealtimeConnected])

  // Desconectar
  const disconnect = useCallback(() => {
    console.log('ðŸ”Œ Desconectando transcriÃ§Ã£o...')
    audioControl.stopAudioCapture()
    setIsConnected(false)
    setRealtimeConnected(false)
  }, [audioControl, setRealtimeConnected])

  return {
    connect,
    disconnect,
    isConnected,
    useOpenAI,
    audioLevel: audioControl.audioLevel,
    devices: audioControl.devices,
    selectedDevice: audioControl.selectedDevice,
    setSelectedDevice: audioControl.setSelectedDevice,
    isSupported: () => typeof window !== 'undefined' && 
                      ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window)
  }
}
